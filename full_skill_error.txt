============================= test session starts =============================
platform win32 -- Python 3.11.8, pytest-8.4.2, pluggy-1.6.0
rootdir: C:\Users\Usuario\Workspace\01_Proyectos\anclora-nexus\sdd\features\intelligence\tests\test-code
configfile: pytest.ini
plugins: anyio-4.11.0, langsmith-0.6.8, asyncio-1.2.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function
collected 25 items

sdd\features\intelligence\tests\test-code\test_skills.py FF.FFFF..FFF... [ 60%]
..........                                                               [100%]

================================== FAILURES ===================================
_________________________ test_lead_intake_happy_path _________________________

mock_llm_service = <MagicMock id='2715375109520'>
mock_supabase_service = <MagicMock id='2715375116048'>

    @pytest.mark.asyncio
    async def test_lead_intake_happy_path(mock_llm_service, mock_supabase_service):
        """TC-SKL-LI-01: Success with complete data."""
        data = {
            "name": "Juan Perez",
            "email": "juan@example.com",
            "phone": "+34600000000",
            "property_interest": "Villa in Calvia",
            "budget": "2M",
            "org_id": "test-org"
        }
    
>       result = await run_lead_intake(data, mock_llm_service, mock_supabase_service)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\tests\test-code\test_skills.py:25: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {'budget': '2M', 'email': 'juan@example.com', 'name': 'Juan Perez', 'org_id': 'test-org', ...}
llm = <MagicMock id='2715375109520'>, db = <MagicMock id='2715375116048'>

    async def run_lead_intake(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for processing new real estate leads.
    
        1. Validates input data with Pydantic.
        2. Summarizes and prioritizes lead with GPT-4o-mini.
        3. Generates luxury-toned drafts with Claude 3.5 Sonnet.
        4. Calculates next actions and due dates.
        5. Returns validated structured results.
    
        Args:
            data: Raw lead dictionary.
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching LeadOutput schema.
    
        Raises:
            ValidationError: If input or output schemas fail.
            Exception: For general processing failures.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"input_summary": data.get("name")})
    
        try:
            # 1. Input Validation
            try:
                validated_input = LeadInput(**data)
            except ValidationError as e:
                log_event("ERROR", "input_validation_failed", {"errors": e.errors()})
                raise
    
            # 2. Analysis (GPT-4o-mini)
            analysis_prompt = f"""
            Analyze this luxury real estate lead:
>           {validated_input.json(ensure_ascii=False)}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
            Provide a JSON response with:
            - "summary": A 3-line luxury summary in Spanish.
            - "priority": An integer 1-5 (5 is highest).
            - "score": A float 0.0-1.0 based on budget, urgency, and fit.
            """

sdd\features\intelligence\skills\lead_intake.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LeadInput(name='Juan Perez', email='juan@example.com', phone='+34600000000', property_interest='Villa in Calvia', budget='2M', source='manual', org_id='test-org')
include = None, exclude = None, by_alias = False, exclude_unset = False
exclude_defaults = False, exclude_none = False, encoder = PydanticUndefined
models_as_dict = PydanticUndefined

    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)
    def json(  # noqa: D102
        self,
        *,
        include: IncEx | None = None,
        exclude: IncEx | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]
        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]
        **dumps_kwargs: Any,
    ) -> str:
        warnings.warn(
            'The `json` method is deprecated; use `model_dump_json` instead.',
            category=PydanticDeprecatedSince20,
            stacklevel=2,
        )
        if encoder is not PydanticUndefined:
            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')
        if models_as_dict is not PydanticUndefined:
            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')
        if dumps_kwargs:
>           raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')
E           TypeError: `dumps_kwargs` keyword arguments are no longer supported.

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:1317: TypeError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.170085", "level": "INFO", "event_type": "skill_started", "skill": "lead_intake", "version": "1.0.0", "details": {"input_summary": "Juan Perez"}}
{"timestamp": "2026-02-13T20:48:26.173497", "level": "CRITICAL", "event_type": "skill_failed", "skill": "lead_intake", "version": "1.0.0", "details": {"error": "`dumps_kwargs` keyword arguments are no longer supported."}}
_______________________ test_lead_intake_high_priority ________________________

mock_llm_service = <MagicMock id='2715374897616'>
mock_supabase_service = <MagicMock id='2715376486160'>

    @pytest.mark.asyncio
    async def test_lead_intake_high_priority(mock_llm_service, mock_supabase_service):
        """TC-SKL-LI-02: Priority assignment based on LLM output."""
        mock_llm_service.analyze.return_value = json.dumps({
            "summary": "High value lead",
            "priority": 5,
            "score": 0.95
        })
    
        data = {"name": "Rich Client", "email": "rich@crypto.com", "budget": "10M"}
>       result = await run_lead_intake(data, mock_llm_service, mock_supabase_service)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\tests\test-code\test_skills.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {'budget': '10M', 'email': 'rich@crypto.com', 'name': 'Rich Client'}
llm = <MagicMock id='2715374897616'>, db = <MagicMock id='2715376486160'>

    async def run_lead_intake(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for processing new real estate leads.
    
        1. Validates input data with Pydantic.
        2. Summarizes and prioritizes lead with GPT-4o-mini.
        3. Generates luxury-toned drafts with Claude 3.5 Sonnet.
        4. Calculates next actions and due dates.
        5. Returns validated structured results.
    
        Args:
            data: Raw lead dictionary.
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching LeadOutput schema.
    
        Raises:
            ValidationError: If input or output schemas fail.
            Exception: For general processing failures.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"input_summary": data.get("name")})
    
        try:
            # 1. Input Validation
            try:
                validated_input = LeadInput(**data)
            except ValidationError as e:
                log_event("ERROR", "input_validation_failed", {"errors": e.errors()})
                raise
    
            # 2. Analysis (GPT-4o-mini)
            analysis_prompt = f"""
            Analyze this luxury real estate lead:
>           {validated_input.json(ensure_ascii=False)}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
            Provide a JSON response with:
            - "summary": A 3-line luxury summary in Spanish.
            - "priority": An integer 1-5 (5 is highest).
            - "score": A float 0.0-1.0 based on budget, urgency, and fit.
            """

sdd\features\intelligence\skills\lead_intake.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LeadInput(name='Rich Client', email='rich@crypto.com', phone=None, property_interest=None, budget='10M', source='manual', org_id=None)
include = None, exclude = None, by_alias = False, exclude_unset = False
exclude_defaults = False, exclude_none = False, encoder = PydanticUndefined
models_as_dict = PydanticUndefined

    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)
    def json(  # noqa: D102
        self,
        *,
        include: IncEx | None = None,
        exclude: IncEx | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]
        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]
        **dumps_kwargs: Any,
    ) -> str:
        warnings.warn(
            'The `json` method is deprecated; use `model_dump_json` instead.',
            category=PydanticDeprecatedSince20,
            stacklevel=2,
        )
        if encoder is not PydanticUndefined:
            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')
        if models_as_dict is not PydanticUndefined:
            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')
        if dumps_kwargs:
>           raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')
E           TypeError: `dumps_kwargs` keyword arguments are no longer supported.

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:1317: TypeError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.202778", "level": "INFO", "event_type": "skill_started", "skill": "lead_intake", "version": "1.0.0", "details": {"input_summary": "Rich Client"}}
{"timestamp": "2026-02-13T20:48:26.204064", "level": "CRITICAL", "event_type": "skill_failed", "skill": "lead_intake", "version": "1.0.0", "details": {"error": "`dumps_kwargs` keyword arguments are no longer supported."}}
_________________________ test_lead_intake_llm_retry __________________________

mock_llm_service = <MagicMock id='2715376659856'>
mock_supabase_service = <MagicMock id='2715376804432'>

    @pytest.mark.asyncio
    async def test_lead_intake_llm_retry(mock_llm_service, mock_supabase_service):
        """Test LLM retry logic on temporary failure."""
        mock_llm_service.analyze.side_effect = [Exception("Timeout"), '{"summary": "ok", "priority": 1, "score": 0.1}']
    
        data = {"name": "Retry User", "email": "retry@test.com"}
>       result = await run_lead_intake(data, mock_llm_service, mock_supabase_service)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\tests\test-code\test_skills.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {'email': 'retry@test.com', 'name': 'Retry User'}
llm = <MagicMock id='2715376659856'>, db = <MagicMock id='2715376804432'>

    async def run_lead_intake(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for processing new real estate leads.
    
        1. Validates input data with Pydantic.
        2. Summarizes and prioritizes lead with GPT-4o-mini.
        3. Generates luxury-toned drafts with Claude 3.5 Sonnet.
        4. Calculates next actions and due dates.
        5. Returns validated structured results.
    
        Args:
            data: Raw lead dictionary.
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching LeadOutput schema.
    
        Raises:
            ValidationError: If input or output schemas fail.
            Exception: For general processing failures.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"input_summary": data.get("name")})
    
        try:
            # 1. Input Validation
            try:
                validated_input = LeadInput(**data)
            except ValidationError as e:
                log_event("ERROR", "input_validation_failed", {"errors": e.errors()})
                raise
    
            # 2. Analysis (GPT-4o-mini)
            analysis_prompt = f"""
            Analyze this luxury real estate lead:
>           {validated_input.json(ensure_ascii=False)}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
            Provide a JSON response with:
            - "summary": A 3-line luxury summary in Spanish.
            - "priority": An integer 1-5 (5 is highest).
            - "score": A float 0.0-1.0 based on budget, urgency, and fit.
            """

sdd\features\intelligence\skills\lead_intake.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LeadInput(name='Retry User', email='retry@test.com', phone=None, property_interest=None, budget=None, source='manual', org_id=None)
include = None, exclude = None, by_alias = False, exclude_unset = False
exclude_defaults = False, exclude_none = False, encoder = PydanticUndefined
models_as_dict = PydanticUndefined

    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)
    def json(  # noqa: D102
        self,
        *,
        include: IncEx | None = None,
        exclude: IncEx | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]
        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]
        **dumps_kwargs: Any,
    ) -> str:
        warnings.warn(
            'The `json` method is deprecated; use `model_dump_json` instead.',
            category=PydanticDeprecatedSince20,
            stacklevel=2,
        )
        if encoder is not PydanticUndefined:
            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')
        if models_as_dict is not PydanticUndefined:
            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')
        if dumps_kwargs:
>           raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')
E           TypeError: `dumps_kwargs` keyword arguments are no longer supported.

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:1317: TypeError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.371052", "level": "INFO", "event_type": "skill_started", "skill": "lead_intake", "version": "1.0.0", "details": {"input_summary": "Retry User"}}
{"timestamp": "2026-02-13T20:48:26.371052", "level": "CRITICAL", "event_type": "skill_failed", "skill": "lead_intake", "version": "1.0.0", "details": {"error": "`dumps_kwargs` keyword arguments are no longer supported."}}
____________________ test_lead_intake_llm_parsing_fallback ____________________

mock_llm_service = <MagicMock id='2715377195984'>
mock_supabase_service = <MagicMock id='2715375122000'>

    @pytest.mark.asyncio
    async def test_lead_intake_llm_parsing_fallback(mock_llm_service, mock_supabase_service):
        """TC-SKL-LI-05: Handle malformed JSON from LLM gracefully."""
        mock_llm_service.analyze.return_value = "This is not JSON"
    
        data = {"name": "Parsing Fail", "email": "fail@test.com"}
>       result = await run_lead_intake(data, mock_llm_service, mock_supabase_service)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\tests\test-code\test_skills.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {'email': 'fail@test.com', 'name': 'Parsing Fail'}
llm = <MagicMock id='2715377195984'>, db = <MagicMock id='2715375122000'>

    async def run_lead_intake(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for processing new real estate leads.
    
        1. Validates input data with Pydantic.
        2. Summarizes and prioritizes lead with GPT-4o-mini.
        3. Generates luxury-toned drafts with Claude 3.5 Sonnet.
        4. Calculates next actions and due dates.
        5. Returns validated structured results.
    
        Args:
            data: Raw lead dictionary.
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching LeadOutput schema.
    
        Raises:
            ValidationError: If input or output schemas fail.
            Exception: For general processing failures.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"input_summary": data.get("name")})
    
        try:
            # 1. Input Validation
            try:
                validated_input = LeadInput(**data)
            except ValidationError as e:
                log_event("ERROR", "input_validation_failed", {"errors": e.errors()})
                raise
    
            # 2. Analysis (GPT-4o-mini)
            analysis_prompt = f"""
            Analyze this luxury real estate lead:
>           {validated_input.json(ensure_ascii=False)}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
            Provide a JSON response with:
            - "summary": A 3-line luxury summary in Spanish.
            - "priority": An integer 1-5 (5 is highest).
            - "score": A float 0.0-1.0 based on budget, urgency, and fit.
            """

sdd\features\intelligence\skills\lead_intake.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LeadInput(name='Parsing Fail', email='fail@test.com', phone=None, property_interest=None, budget=None, source='manual', org_id=None)
include = None, exclude = None, by_alias = False, exclude_unset = False
exclude_defaults = False, exclude_none = False, encoder = PydanticUndefined
models_as_dict = PydanticUndefined

    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)
    def json(  # noqa: D102
        self,
        *,
        include: IncEx | None = None,
        exclude: IncEx | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]
        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]
        **dumps_kwargs: Any,
    ) -> str:
        warnings.warn(
            'The `json` method is deprecated; use `model_dump_json` instead.',
            category=PydanticDeprecatedSince20,
            stacklevel=2,
        )
        if encoder is not PydanticUndefined:
            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')
        if models_as_dict is not PydanticUndefined:
            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')
        if dumps_kwargs:
>           raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')
E           TypeError: `dumps_kwargs` keyword arguments are no longer supported.

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:1317: TypeError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.401607", "level": "INFO", "event_type": "skill_started", "skill": "lead_intake", "version": "1.0.0", "details": {"input_summary": "Parsing Fail"}}
{"timestamp": "2026-02-13T20:48:26.401607", "level": "CRITICAL", "event_type": "skill_failed", "skill": "lead_intake", "version": "1.0.0", "details": {"error": "`dumps_kwargs` keyword arguments are no longer supported."}}
__________________ test_lead_intake_copy_generation_fallback __________________

mock_llm_service = <MagicMock id='2715377382992'>
mock_supabase_service = <MagicMock id='2715377018320'>

    @pytest.mark.asyncio
    async def test_lead_intake_copy_generation_fallback(mock_llm_service, mock_supabase_service):
        """Test copy generation fallback when LLM fails all retries."""
        mock_llm_service.generate_copy.side_effect = Exception("Anthropic Down")
    
        data = {"name": "Copy Fail", "email": "copy@test.com"}
>       result = await run_lead_intake(data, mock_llm_service, mock_supabase_service)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\tests\test-code\test_skills.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {'email': 'copy@test.com', 'name': 'Copy Fail'}
llm = <MagicMock id='2715377382992'>, db = <MagicMock id='2715377018320'>

    async def run_lead_intake(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for processing new real estate leads.
    
        1. Validates input data with Pydantic.
        2. Summarizes and prioritizes lead with GPT-4o-mini.
        3. Generates luxury-toned drafts with Claude 3.5 Sonnet.
        4. Calculates next actions and due dates.
        5. Returns validated structured results.
    
        Args:
            data: Raw lead dictionary.
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching LeadOutput schema.
    
        Raises:
            ValidationError: If input or output schemas fail.
            Exception: For general processing failures.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"input_summary": data.get("name")})
    
        try:
            # 1. Input Validation
            try:
                validated_input = LeadInput(**data)
            except ValidationError as e:
                log_event("ERROR", "input_validation_failed", {"errors": e.errors()})
                raise
    
            # 2. Analysis (GPT-4o-mini)
            analysis_prompt = f"""
            Analyze this luxury real estate lead:
>           {validated_input.json(ensure_ascii=False)}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
            Provide a JSON response with:
            - "summary": A 3-line luxury summary in Spanish.
            - "priority": An integer 1-5 (5 is highest).
            - "score": A float 0.0-1.0 based on budget, urgency, and fit.
            """

sdd\features\intelligence\skills\lead_intake.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LeadInput(name='Copy Fail', email='copy@test.com', phone=None, property_interest=None, budget=None, source='manual', org_id=None)
include = None, exclude = None, by_alias = False, exclude_unset = False
exclude_defaults = False, exclude_none = False, encoder = PydanticUndefined
models_as_dict = PydanticUndefined

    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)
    def json(  # noqa: D102
        self,
        *,
        include: IncEx | None = None,
        exclude: IncEx | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]
        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]
        **dumps_kwargs: Any,
    ) -> str:
        warnings.warn(
            'The `json` method is deprecated; use `model_dump_json` instead.',
            category=PydanticDeprecatedSince20,
            stacklevel=2,
        )
        if encoder is not PydanticUndefined:
            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')
        if models_as_dict is not PydanticUndefined:
            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')
        if dumps_kwargs:
>           raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')
E           TypeError: `dumps_kwargs` keyword arguments are no longer supported.

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:1317: TypeError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.428205", "level": "INFO", "event_type": "skill_started", "skill": "lead_intake", "version": "1.0.0", "details": {"input_summary": "Copy Fail"}}
{"timestamp": "2026-02-13T20:48:26.429261", "level": "CRITICAL", "event_type": "skill_failed", "skill": "lead_intake", "version": "1.0.0", "details": {"error": "`dumps_kwargs` keyword arguments are no longer supported."}}
_____________________ test_lead_intake_audit_log_payload ______________________

mock_llm_service = <MagicMock id='2715377215120'>
mock_supabase_service = <MagicMock id='2715377727440'>

    @pytest.mark.asyncio
    async def test_lead_intake_audit_log_payload(mock_llm_service, mock_supabase_service):
        """TC-SKL-LI-07: Verify logging happens (via prints in this impl)."""
        # Simply check it runs without crashing when logging
        data = {"name": "Log Test", "email": "log@test.com"}
>       await run_lead_intake(data, mock_llm_service, mock_supabase_service)

sdd\features\intelligence\tests\test-code\test_skills.py:92: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {'email': 'log@test.com', 'name': 'Log Test'}
llm = <MagicMock id='2715377215120'>, db = <MagicMock id='2715377727440'>

    async def run_lead_intake(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for processing new real estate leads.
    
        1. Validates input data with Pydantic.
        2. Summarizes and prioritizes lead with GPT-4o-mini.
        3. Generates luxury-toned drafts with Claude 3.5 Sonnet.
        4. Calculates next actions and due dates.
        5. Returns validated structured results.
    
        Args:
            data: Raw lead dictionary.
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching LeadOutput schema.
    
        Raises:
            ValidationError: If input or output schemas fail.
            Exception: For general processing failures.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"input_summary": data.get("name")})
    
        try:
            # 1. Input Validation
            try:
                validated_input = LeadInput(**data)
            except ValidationError as e:
                log_event("ERROR", "input_validation_failed", {"errors": e.errors()})
                raise
    
            # 2. Analysis (GPT-4o-mini)
            analysis_prompt = f"""
            Analyze this luxury real estate lead:
>           {validated_input.json(ensure_ascii=False)}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
            Provide a JSON response with:
            - "summary": A 3-line luxury summary in Spanish.
            - "priority": An integer 1-5 (5 is highest).
            - "score": A float 0.0-1.0 based on budget, urgency, and fit.
            """

sdd\features\intelligence\skills\lead_intake.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = LeadInput(name='Log Test', email='log@test.com', phone=None, property_interest=None, budget=None, source='manual', org_id=None)
include = None, exclude = None, by_alias = False, exclude_unset = False
exclude_defaults = False, exclude_none = False, encoder = PydanticUndefined
models_as_dict = PydanticUndefined

    @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.', category=None)
    def json(  # noqa: D102
        self,
        *,
        include: IncEx | None = None,
        exclude: IncEx | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]
        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]
        **dumps_kwargs: Any,
    ) -> str:
        warnings.warn(
            'The `json` method is deprecated; use `model_dump_json` instead.',
            category=PydanticDeprecatedSince20,
            stacklevel=2,
        )
        if encoder is not PydanticUndefined:
            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')
        if models_as_dict is not PydanticUndefined:
            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')
        if dumps_kwargs:
>           raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')
E           TypeError: `dumps_kwargs` keyword arguments are no longer supported.

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:1317: TypeError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.455775", "level": "INFO", "event_type": "skill_started", "skill": "lead_intake", "version": "1.0.0", "details": {"input_summary": "Log Test"}}
{"timestamp": "2026-02-13T20:48:26.456352", "level": "CRITICAL", "event_type": "skill_failed", "skill": "lead_intake", "version": "1.0.0", "details": {"error": "`dumps_kwargs` keyword arguments are no longer supported."}}
__________________________ test_prospection_no_leads __________________________

mock_llm_service = <MagicMock id='2715377642448'>
mock_supabase_service = <MagicMock id='2715376961296'>

    @pytest.mark.asyncio
    async def test_prospection_no_leads(mock_llm_service, mock_supabase_service):
        """TC-SKL-PW-08: Handle no active leads."""
        mock_supabase_service.get_active_leads.return_value = []
    
        result = await run_prospection_weekly({}, mock_llm_service, mock_supabase_service)
        assert result["status"] == "skipped"
>       assert "no_leads" in result["reason"].lower()
E       AssertionError: assert 'no_leads' in 'no active leads found with required priority.'
E        +  where 'no active leads found with required priority.' = <built-in method lower of str object at 0x0000027838F2A1F0>()
E        +    where <built-in method lower of str object at 0x0000027838F2A1F0> = 'No active leads found with required priority.'.lower

sdd\features\intelligence\tests\test-code\test_skills.py:125: AssertionError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.524070", "level": "INFO", "event_type": "skill_started", "skill": "prospection_weekly", "version": "1.0.0", "details": {"data": {}}}
{"timestamp": "2026-02-13T20:48:26.524735", "level": "INFO", "event_type": "prospection_skipped", "skill": "prospection_weekly", "version": "1.0.0", "details": {"reason": "no_leads"}}
_______________________ test_prospection_no_properties ________________________

mock_llm_service = <MagicMock id='2715377565008'>
mock_supabase_service = <MagicMock id='2715377593360'>

    @pytest.mark.asyncio
    async def test_prospection_no_properties(mock_llm_service, mock_supabase_service):
        """TC-SKL-PW-08: Handle no properties."""
        mock_supabase_service.get_available_properties.return_value = []
    
        result = await run_prospection_weekly({}, mock_llm_service, mock_supabase_service)
        assert result["status"] == "skipped"
>       assert "no_properties" in result["reason"].lower()
E       AssertionError: assert 'no_properties' in 'no available properties found for matching.'
E        +  where 'no available properties found for matching.' = <built-in method lower of str object at 0x0000027838F29E90>()
E        +    where <built-in method lower of str object at 0x0000027838F29E90> = 'No available properties found for matching.'.lower

sdd\features\intelligence\tests\test-code\test_skills.py:134: AssertionError
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.548484", "level": "INFO", "event_type": "skill_started", "skill": "prospection_weekly", "version": "1.0.0", "details": {"data": {}}}
{"timestamp": "2026-02-13T20:48:26.549156", "level": "INFO", "event_type": "prospection_skipped", "skill": "prospection_weekly", "version": "1.0.0", "details": {"reason": "no_properties"}}
____________________ test_prospection_llm_timeout_fallback ____________________

mock_llm_service = <MagicMock id='2715377335440'>
mock_supabase_service = <MagicMock id='2715377245904'>

    @pytest.mark.asyncio
    async def test_prospection_llm_timeout_fallback(mock_llm_service, mock_supabase_service):
        """TC-SKL-PW-06: LLM timeout on matching."""
        mock_llm_service.analyze.side_effect = Exception("LLM Timeout")
    
>       result = await run_prospection_weekly({}, mock_llm_service, mock_supabase_service)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\tests\test-code\test_skills.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = {}, llm = <MagicMock id='2715377335440'>
db = <MagicMock id='2715377245904'>

    async def run_prospection_weekly(data: Dict[str, Any], llm: LLMService, db: SupabaseService) -> Dict[str, Any]:
        """
        Skill for weekly property prospection and lead matching.
    
        1. Fetches high-priority leads and available properties from Supabase.
        2. Uses GPT-4o-mini to find logical matches between leads and properties.
        3. Generates a luxury executive summary of the prospection with Claude 3.5 Sonnet.
        4. Validates all inputs and outputs with Pydantic.
    
        Args:
            data: Execution parameters (e.g., priority_min).
            llm: Injected LLM service.
            db: Injected Supabase service.
    
        Returns:
            A dictionary matching ProspectionOutput schema.
        """
        start_time = datetime.utcnow()
        log_event("INFO", "skill_started", {"data": data})
    
        try:
            # 1. Input Validation
            params = ProspectionInput(**data)
    
            # 2. Fetch Context
            leads = await db.get_active_leads(priority_min=params.priority_min)
            properties = await db.get_available_properties()
    
            if not leads:
                log_event("INFO", "prospection_skipped", {"reason": "no_leads"})
                return {"status": "skipped", "reason": "No active leads found with required priority."}
    
            if not properties:
                log_event("INFO", "prospection_skipped", {"reason": "no_properties"})
                return {"status": "skipped", "reason": "No available properties found for matching."}
    
            # 3. Property Matching (GPT-4o-mini)
            matching_prompt = f"""
            Eres un experto inmobiliario de lujo en Mallorca.
            Tu tarea es cruzar estos LEADS con estas PROPIEDADES disponibles.
    
            LEADS:
            {json.dumps([{ 'id': l['id'], 'name': l['name'], 'interest': l['property_interest'], 'budget': l['budget_range']} for l in leads], ensure_ascii=False)}
    
            PROPIEDADES:
            {json.dumps([{ 'id': p['id'], 'address': p['address'], 'price': p['price'], 'type': p['property_type']} for p in properties], ensure_ascii=False)}
    
            Para cada LEAD, encuentra la mejor propiedad coincidente (si hay alguna razonable).
    
            Responde UNICAMENTE con un JSON:
            {{
              "matchings": [
                {{
                  "lead_id": "uuid",
                  "property_id": "uuid",
                  "score": 0.0-1.0,
                  "reason": "breve explicaci�n de por qu� encaja"
                }}
              ]
            }}
            """
    
>           matching_raw = await _call_llm_with_retry(llm.analyze, matching_prompt)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\skills\prospection_weekly.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

func = <AsyncMock name='mock.analyze' id='2715377546512'>, attempts = 3
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}, last_error = Exception('LLM Timeout'), i = 2, wait_time = 2.0

    async def _call_llm_with_retry(func, *args, attempts: int = 3, **kwargs) -> Any:
        """Retry logic for LLM calls with exponential backoff."""
        last_error = None
        for i in range(attempts):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_error = e
                wait_time = (2 ** i) * 0.5
                log_event("WARNING", "llm_retry", {"attempt": i + 1, "error": str(e)})
                await asyncio.sleep(wait_time)
>       raise last_error

sdd\features\intelligence\skills\prospection_weekly.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

func = <AsyncMock name='mock.analyze' id='2715377546512'>, attempts = 3
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}, last_error = Exception('LLM Timeout'), i = 2, wait_time = 2.0

    async def _call_llm_with_retry(func, *args, attempts: int = 3, **kwargs) -> Any:
        """Retry logic for LLM calls with exponential backoff."""
        last_error = None
        for i in range(attempts):
            try:
>               return await func(*args, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\skills\prospection_weekly.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='mock.analyze' id='2715377546512'>
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}
_call = call('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PR...-1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ')
effect = Exception('LLM Timeout')

    async def _execute_mock_call(self, /, *args, **kwargs):
        # This is nearly just like super(), except for special handling
        # of coroutines
    
        _call = _Call((args, kwargs), two=True)
        self.await_count += 1
        self.await_args = _call
        self.await_args_list.append(_call)
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect

..\..\..\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:2237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

func = <AsyncMock name='mock.analyze' id='2715377546512'>, attempts = 3
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}, last_error = Exception('LLM Timeout'), i = 2, wait_time = 2.0

    async def _call_llm_with_retry(func, *args, attempts: int = 3, **kwargs) -> Any:
        """Retry logic for LLM calls with exponential backoff."""
        last_error = None
        for i in range(attempts):
            try:
>               return await func(*args, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\skills\prospection_weekly.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='mock.analyze' id='2715377546512'>
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}
_call = call('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PR...-1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ')
effect = Exception('LLM Timeout')

    async def _execute_mock_call(self, /, *args, **kwargs):
        # This is nearly just like super(), except for special handling
        # of coroutines
    
        _call = _Call((args, kwargs), two=True)
        self.await_count += 1
        self.await_args = _call
        self.await_args_list.append(_call)
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect

..\..\..\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:2237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

func = <AsyncMock name='mock.analyze' id='2715377546512'>, attempts = 3
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}, last_error = Exception('LLM Timeout'), i = 2, wait_time = 2.0

    async def _call_llm_with_retry(func, *args, attempts: int = 3, **kwargs) -> Any:
        """Retry logic for LLM calls with exponential backoff."""
        last_error = None
        for i in range(attempts):
            try:
>               return await func(*args, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sdd\features\intelligence\skills\prospection_weekly.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='mock.analyze' id='2715377546512'>
args = ('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PROPIE...1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ',)
kwargs = {}
_call = call('\n        Eres un experto inmobiliario de lujo en Mallorca.\n        Tu tarea es cruzar estos LEADS con estas PR...-1.0,\n              "reason": "breve explicaci�n de por qu� encaja"\n            }\n          ]\n        }\n        ')
effect = Exception('LLM Timeout')

    async def _execute_mock_call(self, /, *args, **kwargs):
        # This is nearly just like super(), except for special handling
        # of coroutines
    
        _call = _Call((args, kwargs), two=True)
        self.await_count += 1
        self.await_args = _call
        self.await_args_list.append(_call)
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect
E               Exception: LLM Timeout

..\..\..\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:2237: Exception
---------------------------- Captured stdout call -----------------------------
{"timestamp": "2026-02-13T20:48:26.564576", "level": "INFO", "event_type": "skill_started", "skill": "prospection_weekly", "version": "1.0.0", "details": {"data": {}}}
{"timestamp": "2026-02-13T20:48:26.564576", "level": "WARNING", "event_type": "llm_retry", "skill": "prospection_weekly", "version": "1.0.0", "details": {"attempt": 1, "error": "LLM Timeout"}}
{"timestamp": "2026-02-13T20:48:27.072176", "level": "WARNING", "event_type": "llm_retry", "skill": "prospection_weekly", "version": "1.0.0", "details": {"attempt": 2, "error": "LLM Timeout"}}
{"timestamp": "2026-02-13T20:48:28.072843", "level": "WARNING", "event_type": "llm_retry", "skill": "prospection_weekly", "version": "1.0.0", "details": {"attempt": 3, "error": "LLM Timeout"}}
{"timestamp": "2026-02-13T20:48:30.073541", "level": "CRITICAL", "event_type": "skill_failed", "skill": "prospection_weekly", "version": "1.0.0", "details": {"error": "LLM Timeout"}}
============================== warnings summary ===============================
backend\intelligence\models.py:16
  C:\Users\Usuario\Workspace\01_Proyectos\anclora-nexus\backend\intelligence\models.py:16: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

..\..\..\AppData\Local\Programs\Python\Python311\Lib\site-packages\_pytest\config\__init__.py:1474
  C:\Users\Usuario\AppData\Local\Programs\Python\Python311\Lib\site-packages\_pytest\config\__init__.py:1474: PytestConfigWarning: Unknown config option: env
  
    self._warn_or_fail_if_strict(f"Unknown config option: {key}\n")

test_skills.py::test_lead_intake_happy_path
test_skills.py::test_lead_intake_high_priority
test_skills.py::test_lead_intake_llm_retry
test_skills.py::test_lead_intake_llm_parsing_fallback
test_skills.py::test_lead_intake_copy_generation_fallback
test_skills.py::test_lead_intake_audit_log_payload
test_skills.py::test_lead_intake_critical_failure
  C:\Users\Usuario\Workspace\01_Proyectos\anclora-nexus\sdd\features\intelligence\skills\lead_intake.py:118: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    {validated_input.json(ensure_ascii=False)}

test_skills.py::test_prospection_happy_path
test_skills.py::test_prospection_complex_matching
test_skills.py::test_prospection_complex_matching
  C:\Users\Usuario\Workspace\01_Proyectos\anclora-nexus\sdd\features\intelligence\skills\prospection_weekly.py:155: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    {json.dumps([m.dict() for m in matchings], ensure_ascii=False)}

test_skills.py::test_prospection_happy_path
test_skills.py::test_prospection_matching_parsing_error
test_skills.py::test_prospection_complex_matching
test_skills.py::test_prospection_supabase_call
  C:\Users\Usuario\Workspace\01_Proyectos\anclora-nexus\sdd\features\intelligence\skills\prospection_weekly.py:177: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    return result.dict()

test_skills.py::test_recap_happy_path
test_skills.py::test_recap_empty_data
test_skills.py::test_recap_db_failure_graceful
test_skills.py::test_recap_llm_fallback
test_skills.py::test_recap_high_priority_highlight
test_skills.py::test_recap_execution_metric_calc
test_skills.py::test_recap_duration_logging
  C:\Users\Usuario\Workspace\01_Proyectos\anclora-nexus\sdd\features\intelligence\skills\recap_weekly.py:185: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    return result.dict()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_lead_intake_happy_path
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_lead_intake_high_priority
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_lead_intake_llm_retry
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_lead_intake_llm_parsing_fallback
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_lead_intake_copy_generation_fallback
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_lead_intake_audit_log_payload
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_prospection_no_leads
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_prospection_no_properties
FAILED sdd\features\intelligence\tests\test-code\test_skills.py::test_prospection_llm_timeout_fallback
================= 9 failed, 16 passed, 23 warnings in 13.80s ==================
